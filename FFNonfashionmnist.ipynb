{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory application of neural networks to data analysis\n",
    "\n",
    "In this notebook we give a condensed intro to the application of feed forward networks to a common data set without discussing deeply the theory and motivation. We try different network complexities, optimizers and activation functions to get a feeling for how they impact the minimization of the loss/error/cost function. Regularization techniques are not discussed here since we do not train for too many epochs and the overall scores are not too good to justify thir application to counter overfitting. For further discussion on different network architectures, we refer to repositories [here](https://github.com/andreaspts/DL_DEEPNET_vs_CONVNET_on_MNIST) and [here](https://github.com/andreaspts/DL_REC_vs_DEEP_and_CONVNN_on_TEMPERATURE_SERIES)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant data\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define data \n",
    "X = [[50], [60], [70], [20], [10], [30]]\n",
    "\n",
    "Y = [1,1,1,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andreas/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C = 100000) #large C dampens regulatization\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[44]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08358881, 0.91641119]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba([[44]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple neural network on fashion mnist\n",
    "\n",
    "In the following we will discuss the application of simple feed forward neural networks onto the fashion mnist data set to classify item categories. The network architectures we use are implemented conveniently via keras. The data set can be retrieved from [here](https://github.com/zalandoresearch/fashion-mnist). A score board comparing different ml methods using classical scikit-learn algorithms and neural networks is found [here](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip \n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_images(filename):\n",
    "    with gzip.open(filename, \"rb\") as file:\n",
    "        data = file.read()\n",
    "        return np.frombuffer(data, dtype = np.uint8, offset = 16)\\\n",
    "            .reshape(-1, 28, 28)\\\n",
    "            .astype(np.float32)\n",
    "    \n",
    "def open_labels(filename):\n",
    "    with gzip.open(filename, \"rb\") as file:\n",
    "        data = file.read()\n",
    "        return np.frombuffer(data, dtype = np.uint8, offset = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and define variables\n",
    "X_train = open_images(\"train-images-idx3-ubyte.gz\")\n",
    "Y_train = open_labels(\"train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "Y_train = (Y_train == 0) #just checking for the t-shirts\n",
    "\n",
    "X_test = open_images(\"t10k-images-idx3-ubyte.gz\")\n",
    "Y_test = open_labels(\"t10k-labels-idx1-ubyte.gz\")\n",
    "\n",
    "Y_test = (Y_test == 0) #just checking for the t-shirts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[100], cmap = \"gray_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(100, activation = \"sigmoid\", input_shape = (28 * 28,))) # we have 28*28 pixels\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "model.compile(optimizer = \"sgd\", loss = \"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent was employed as optimizer. Below we will use an improved version of gradient descent which tries to smoothen out oscillations in the descending procedure (gd with momentum, rmsprop or adam optimizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"gdanimations.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#illustration of different optimizers\n",
    "from IPython.display import Image\n",
    "Image(url='gdanimations.gif')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "#train on minibatches of 1000 to get an adjustment (thus 60 adjustments per epoch)\n",
    "history = model.fit(X_train.reshape(60000, 784), Y_train, epochs = 10, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check per hand prediction on training data vs. reality\n",
    "plt.imshow(X_train[0], cmap = \"gray_r\")\n",
    "print(Y_train[0])\n",
    "print(model.predict(X_train[0].reshape(1, 784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check per hand prediction on training data vs. reality\n",
    "plt.imshow(X_train[1],cmap = \"gray_r\")\n",
    "print(Y_train[1])\n",
    "print(model.predict(X_train[1].reshape(1, 784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the accuracy (per hand)\n",
    "Y_train_pred = model.predict(X_train.reshape(60000, 784))\n",
    "np.mean(np.round(Y_train_pred).reshape(-1) == Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check accuracy on training set via keras --> use output from fitting process\n",
    "model.evaluate(X_train.reshape(60000, 784), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check accuracy on test set via keras --> use output from fitting process\n",
    "model.evaluate(X_test.reshape(10000, 784), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this aim: Modify final activation function, output dimension and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and define variables\n",
    "X_train = open_images(\"train-images-idx3-ubyte.gz\")\n",
    "Y_train = open_labels(\"train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "X_test = open_images(\"t10k-images-idx3-ubyte.gz\")\n",
    "Y_test = open_labels(\"t10k-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use one-hot-encoding\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model (adapt to categorical situaton)\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(2048, activation = \"sigmoid\", input_shape = (28 * 28,))) # we have 28*28 pixels\n",
    "model.add(layers.Dense(256, activation = \"sigmoid\", input_shape = (28 * 28,))) # we have 28*28 pixels\n",
    "model.add(layers.Dense(10, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "#train on minibatches of 1000 to get an adjustment (thus 60 adjustments per epoch)\n",
    "history = model.fit(X_train.reshape(60000, 784), Y_train, epochs = 10, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check accuracy on training set via keras --> use output from fitting process\n",
    "model.evaluate(X_train.reshape(60000, 784), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check accuracy on test set via keras --> use output from fitting process\n",
    "model.evaluate(X_test.reshape(10000, 784), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation via confusion matrix (helps to see if classes are well discerned)\n",
    "Y_pred = model.predict(X_test.reshape(-1, 784))\n",
    "\n",
    "#check for which category the highest estimate was produced for all examples in the test set\n",
    "np.argmax(Y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrix(np.argmax(Y_test, axis = 1), np.argmax(Y_pred, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example: Column \"2\" line \"0\": 27 is to be read as: Predicted was category \"2\" when it was actually category \"0\". In this way, the confusion matrix allows to see how well our model maps the reality. Ideally, we would like to have a model with vanishing off-diagonal terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing the softmax as final layer activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 2048)              1607680   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 2,134,794\n",
      "Trainable params: 2,134,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define model (adapt to categorical situaton)\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(2048, activation = \"sigmoid\", input_shape = (28 * 28,))) # we have 28*28 pixels\n",
    "model.add(layers.Dense(256, activation = \"sigmoid\", input_shape = (28 * 28,))) # we have 28*28 pixels\n",
    "model.add(layers.Dense(10, activation = \"softmax\"))\n",
    "model.summary()\n",
    "model.compile(optimizer = \"rmsprop\", loss = \"categorical_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 16s 274us/step - loss: 1.0387 - acc: 0.6309\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 18s 302us/step - loss: 0.6311 - acc: 0.7594\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 21s 345us/step - loss: 0.5711 - acc: 0.7834\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 18s 297us/step - loss: 0.5329 - acc: 0.7983\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 17s 288us/step - loss: 0.5058 - acc: 0.8080\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 17s 282us/step - loss: 0.4824 - acc: 0.8174\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 17s 276us/step - loss: 0.4706 - acc: 0.8231\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 17s 283us/step - loss: 0.4557 - acc: 0.8283\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 17s 281us/step - loss: 0.4440 - acc: 0.8348\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 17s 276us/step - loss: 0.4402 - acc: 0.8357\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "#train on minibatches of 1000 to get an adjustment (thus 60 adjustments per epoch)\n",
    "history = model.fit(X_train.reshape(60000, 784), Y_train, epochs = 10, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the tanh and sigmoid activations work better on this problem than relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 10s 172us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40780154851277667, 0.8429333333333333]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check accuracy on training set via keras --> use output from fitting process\n",
    "model.evaluate(X_train.reshape(60000, 784), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 236us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44668299560546876, 0.8317]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check accuracy on test set via keras --> use output from fitting process\n",
    "model.evaluate(X_test.reshape(10000, 784), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, ..., 8, 1, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model evaluation via confusion matrix (helps to see if classes are well discerned)\n",
    "Y_pred = model.predict(X_test.reshape(-1, 784))\n",
    "\n",
    "#check for which category the highest estimate was produced for all examples in the test set\n",
    "np.argmax(Y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predicted    0    1    2     3     4    5    6     7     8    9  __all__\n",
       "Actual                                                                  \n",
       "0          779    3   20    87    13    0   79     0    19    0     1000\n",
       "1            0  959    4    26     9    0    0     0     2    0     1000\n",
       "2            8    1  608    15   325    0   28     0    15    0     1000\n",
       "3           17   10    8   875    67    0   19     0     4    0     1000\n",
       "4            0    0   44    24   917    0   11     0     4    0     1000\n",
       "5            0    0    0     2     0  934    0    46     4   14     1000\n",
       "6          151    3  116    55   262    0  387     0    26    0     1000\n",
       "7            0    0    0     0     0   12    0   952     0   36     1000\n",
       "8            0    1    9     4     6    2    4     4   970    0     1000\n",
       "9            0    0    0     1     0    9    0    53     1  936     1000\n",
       "__all__    955  977  809  1089  1599  957  528  1055  1045  986    10000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConfusionMatrix(np.argmax(Y_test, axis = 1), np.argmax(Y_pred, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We understand from the different scores (training and test scores are pretty close) that by increaing the network capacity more statistical intricacies of the data set could be unveiled. \n",
    "\n",
    "If they were to far from each other (while the training score would be good) the complexity would be to big and we would observe overfitting.\n",
    "\n",
    "If both scores would be bad, more data could help."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
